{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZSdjAgdXcraM"
   },
   "source": [
    "# Speech-based Emotion Recognition for Voice Comms.\n",
    "## Main Notebook\n",
    "\n",
    "Datasets used:\n",
    "\n",
    "*   RAVDESS, CREMA-D\n",
    "*   See AugmentData.ipynb for augmentation scripts  \n",
    "\n",
    "Models:\n",
    "* Pytorch MLP\n",
    "* Example sklearn RandomForest, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GicI2VMDHTni"
   },
   "outputs": [],
   "source": [
    "import soundfile # read audio files\n",
    "import numpy as np\n",
    "import librosa # extract features\n",
    "import glob\n",
    "import os\n",
    "import pickle # to save model after training\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split # for splitting training and testing data\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handling, feature extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndQyj81GIfUI"
   },
   "outputs": [],
   "source": [
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\") # all depth 32bit float\n",
    "        sample_rate = sound_file.samplerate # should always be 16kHz\n",
    "        if chroma or contrast:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "        if contrast:\n",
    "            contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, contrast))\n",
    "        if tonnetz:\n",
    "            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "            result = np.hstack((result, tonnetz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all emotions RAVDESS\n",
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# all emotions CREMA-D\n",
    "cemotion = {\n",
    "    \"NEU\": \"neutral\",\n",
    "    \"HAP\": \"happy\",\n",
    "    \"SAD\": \"sad\",\n",
    "    \"ANG\": \"angry\",\n",
    "    \"FEA\": \"fearful\",\n",
    "    \"DIS\": \"disgust\",\n",
    "}\n",
    "\n",
    "tessemotion = {\n",
    "    \"neutral.wav\": \"neutral\",\n",
    "    \"happy.wav\": \"happy\",\n",
    "    \"sad.wav\": \"sad\",\n",
    "    \"angry.wav\": \"angry\",\n",
    "    \"fear.wav\": \"fearful\",\n",
    "    \"disgust.wav\": \"disgust\",\n",
    "    \"ps.wav\": \"surprised\",\n",
    "}\n",
    "\n",
    "saveeemotion = {\n",
    "    \"n\": \"neutral\",\n",
    "    \"h\": \"happy\",\n",
    "    \"s\": \"sad\",\n",
    "    \"a\": \"angry\",\n",
    "    \"f\": \"fearful\",\n",
    "    \"d\": \"disgust\",\n",
    "    \"p\": \"surprised\",\n",
    "}\n",
    "\n",
    "# allow only these emotions from dataset(s)\n",
    "AVAILABLE_EMOTIONS = {\n",
    "    \"neutral\",\n",
    "    \"calm\", # only included in RAVDESS\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\", \n",
    "    \"fearful\",\n",
    "    \"disgust\",\n",
    "    \"surprised\" # in TESS specifies pleasant surprise\n",
    "}\n",
    "\n",
    "# map emotion to 2D space\n",
    "circumplex_model = {\n",
    "    \"neutral\":[(0.0, 0.0),\"grey\"],\n",
    "    \"calm\":[(0.5, -0.7),\"cyan\"], # only included in RAVDESS\n",
    "    \"happy\":[(0.8, 0.1),\"purple\"],\n",
    "    \"sad\":[(-0.8, -0.3),\"blue\"],\n",
    "    \"angry\":[(-0.2, 0.7),\"red\"],\n",
    "    \"fearful\":[(-0.4, 0.7),\"magenta\"], #circumplex afraid\n",
    "    \"disgust\":[(-0.41, 0.6),\"green\"], #circumplex annoyed\n",
    "    \"surprised\":[(0.3, 0.7),\"orange\"], #circumplex astonished, in TESS specifies pleasant surprise\n",
    "}\n",
    "\n",
    "# map emotion to 3d space\n",
    "pad_model = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Ivlry11IruV"
   },
   "outputs": [],
   "source": [
    "# load data and extract features\n",
    "X, y = [], []\n",
    "\n",
    "## RAVDESS clean\n",
    "for file in glob.glob(\"data/RAVDESS-nosil/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "    # we allow only circumplex model emotions\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "\n",
    "## RAVDESS augmented\n",
    "# white noise\n",
    "for file in glob.glob(\"data/augment-wn/RAVDESS/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "# background noise\n",
    "for file in glob.glob(\"data/augment-bg/RAVDESS/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "# reverb\n",
    "for file in glob.glob(\"data/augment-reverb/RAVDESS/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "\n",
    "# overdrive\n",
    "for file in glob.glob(\"data/augment-overdrive/RAVDESS/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = int2emotion[basename.split(\"-\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "# CREMA-D clean\n",
    "for file in glob.glob(\"data/CREMA-D/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = cemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "## CREMA-D augmented\n",
    "# white noise\n",
    "for file in glob.glob(\"data/augment-wn/CREMA-D/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = cemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "# background noise\n",
    "for file in glob.glob(\"data/augment-bg/CREMA-D/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = cemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "    \n",
    "# reverb\n",
    "for file in glob.glob(\"data/augment-reverb/CREMA-D/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = cemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "\n",
    "# overdrive\n",
    "for file in glob.glob(\"data/augment-overdrive/CREMA-D/*.wav\"):\n",
    "    # get the base name of the audio file\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\n",
    "    emotion = cemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    X.append(features)\n",
    "    y.append(circumplex_coord)\n",
    "\n",
    "# scale features [1, -1]\n",
    "X /= np.max(np.abs(X), axis=0)\n",
    "\n",
    "# train test validation split\n",
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=1 - train_ratio, random_state=420, stratify=y)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=420, stratify=y_test)\n",
    "\n",
    "# # label encoding for y\n",
    "# label_encoder = preprocessing.LabelEncoder()\n",
    "# label_encoder.fit(y)\n",
    "# print(\"Class mapping: \")\n",
    "# for i, item in enumerate(label_encoder.classes_):\n",
    "#     print(item, \"-->\", i)\n",
    "\n",
    "# encoded_labels = label_encoder.transform(y)\n",
    "# print(\"Labels =\", labels)\n",
    "# print(\"Encoded labels =\", list(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom test set load, overwrites\n",
    "Xt, yt = [], []\n",
    "\n",
    "# TESS\n",
    "for file in glob.glob(\"data/TESS/*/*.wav\"):\n",
    "    # get the base name of the audio file\\n\",\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\\n\",\n",
    "    emotion = tessemotion[basename.split(\"_\")[2]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract speech features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    Xt.append(features)\n",
    "    yt.append(circumplex_coord)\n",
    "    \n",
    "# SAVEE\n",
    "for file in glob.glob(\"data/SAVEE/*/*.wav\"):\n",
    "    # get the base name of the audio file\\n\",\n",
    "    basename = os.path.basename(file)\n",
    "    # get the emotion label\\n\",\n",
    "    emotion = saveeemotion[basename[0]]\n",
    "    # we allow only AVAILABLE_EMOTIONS we set\n",
    "    if emotion not in AVAILABLE_EMOTIONS:\n",
    "        continue\n",
    "    # map coords\n",
    "    circumplex_coord = circumplex_model[emotion][0]\n",
    "    # extract speech features\n",
    "    features = extract_feature(file, mfcc=True)\n",
    "    # add to data\n",
    "    Xt.append(features)\n",
    "    yt.append(circumplex_coord)\n",
    "    \n",
    "Xt /= np.max(np.abs(Xt), axis=0)\n",
    "\n",
    "X_bin, X_test, y_bin, y_test = train_test_split(np.array(Xt), np.array(yt), test_size = 0.2, random_state = 420, stratify=yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of available samples\n",
    "print(\"[+] Number of samples:\", X.shape[0])\n",
    "# number of samples in training data\n",
    "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
    "# number of samples in validation data\n",
    "print(\"[+] Number of validation samples:\", X_val.shape[0])\n",
    "# number of samples in test data\n",
    "print(\"[+] Number of test samples:\", X_test.shape[0])\n",
    "# vector of features extracted \n",
    "print(\"[+] Number of features:\", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(data=y_train, columns=['X','Y',])\n",
    "y_train_df['X'].value_counts().plot.bar(title='Train Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_df = pd.DataFrame(data=y_val, columns=['X','Y',])\n",
    "y_val_df['X'].value_counts().plot.bar(title='Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(data=y_test, columns=['X','Y',])\n",
    "y_test_df['X'].value_counts().plot.bar(title='Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining utility class\n",
    "#by defining this, you only have to write \"for loop\" to load minibatch data\n",
    "class DataLoader(object):\n",
    "    def __init__(self, x, y, batch_size, shuffle):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.start_idx = 0\n",
    "        self.data_size = x.shape[0]\n",
    "        if self.shuffle:\n",
    "            self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x, self.y = shuffle(self.x, self.y)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.start_idx >= self.data_size:\n",
    "            if self.shuffle:\n",
    "                self.reset()\n",
    "            self.start_idx = 0\n",
    "            raise StopIteration\n",
    "    \n",
    "        batch_x = self.x[self.start_idx:self.start_idx+self.batch_size]\n",
    "        batch_y = self.y[self.start_idx:self.start_idx+self.batch_size]\n",
    "\n",
    "        batch_x = torch.tensor(batch_x, dtype=torch.float, device=device)\n",
    "        batch_y = torch.tensor(batch_y, dtype=torch.float, device=device)\n",
    "\n",
    "        self.start_idx += self.batch_size\n",
    "\n",
    "        return (batch_x,batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(X_train, y_train,\n",
    "                 batch_size=X_train.shape[0],\n",
    "                 shuffle=False)\n",
    "\n",
    "val_loader = DataLoader(X_val, y_val,\n",
    "                 batch_size=X_val.shape[0],\n",
    "                 shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Checking the dataset\n",
    "for features, labels in train_loader:  \n",
    "    print('features dimensions:', features.shape)\n",
    "    print('label dimensions:', labels.shape)\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "for features, labels in val_loader:  \n",
    "    print('features dimensions:', features.shape)\n",
    "    print('label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining MLP model\n",
    "#generally out_dim is more than 1, but this model only allows 1.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim_1, hidden_dim_2, out_dim=2):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim_1 = hidden_dim_1\n",
    "        self.hidden_dim_2 = hidden_dim_2\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        ### 1st hidden layer\n",
    "        self.linear_1 = nn.Linear(self.in_dim, self.hidden_dim_1)\n",
    "        self.linear_1.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_1.bias.detach().zero_()\n",
    "        self.linear_1_bn = nn.BatchNorm1d(self.hidden_dim_1,momentum=0.6)\n",
    "        \n",
    "        ### 2nd hidden layer\n",
    "        self.linear_2 = nn.Linear(self.hidden_dim_1, self.hidden_dim_2)\n",
    "        self.linear_2.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_2.bias.detach().zero_()\n",
    "        self.linear_2_bn = nn.BatchNorm1d(self.hidden_dim_2,momentum=0.6)\n",
    "        \n",
    "        ## Out layer\n",
    "        self.linear_out = nn.Linear(self.hidden_dim_2, self.out_dim)\n",
    "        self.linear_out.weight.detach().normal_(0.0, 0.1)\n",
    "        self.linear_out.bias.detach().zero_()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = self.linear_1_bn(out)\n",
    "        out = F.relu(out)\n",
    "        #out = F.dropout(out, p=0.5, training=self.training)\n",
    "        \n",
    "        out = self.linear_2(out)\n",
    "        out = self.linear_2_bn(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        \n",
    "        out = self.linear_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model\n",
    "mlp = MLP(40, 300, 150, 2).to(device)\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1, momentum=0.6, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training phase\n",
    "epochs = 100\n",
    "# to plot loss curve after training\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    mlp.train()\n",
    "    num_batch = train_loader.data_size // train_loader.batch_size + 1\n",
    "    \n",
    "    for batch_id, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        \n",
    "        y_pred = mlp(batch_x)\n",
    "\n",
    "        loss = F.mse_loss(y_pred, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_min = int(elapsed_time / 60)\n",
    "        elapsed_sec = elapsed_time - 60 * elapsed_min\n",
    "\n",
    "        print('\\rEpoch:{} Batch:{}/{} Loss:{:.4f} Time:{}m{:.2f}s'.format(epoch + 1, batch_id, \n",
    "                                                                          num_batch, loss.item(),\n",
    "                                                                          elapsed_min, elapsed_sec), end='')\n",
    "    print()\n",
    "    mlp.eval()\n",
    "    valid_loss = 0\n",
    "    best_loss = np.inf\n",
    "    num_batch = val_loader.data_size // val_loader.batch_size + 1\n",
    "    \n",
    "    for batch_id, (batch_x, batch_y) in enumerate(val_loader):\n",
    "    \n",
    "        y_pred = mlp(batch_x)\n",
    "        loss = F.mse_loss(y_pred, batch_y)\n",
    "        valid_loss += loss.item()\n",
    "    \n",
    "    valid_loss /= num_batch\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    #save model when validation loss is minimum\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(mlp.state_dict(), 'mlp.model') \n",
    "    \n",
    "    print('Valid Loss:{:.4f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation loss curve, this may help to notice overfitting\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.ylim(0,max(valid_losses)+0.02)\n",
    "plt.xlim(0,epochs)\n",
    "plt.plot(valid_losses)\n",
    "print('minimum validation loss is {:.4f}'.format(min(valid_losses)))\n",
    "\n",
    "# # sanity\n",
    "# print(y_pred.shape)\n",
    "# print(y_pred[:1])\n",
    "# print(batch_y.shape)\n",
    "# print(batch_y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model\n",
    "mlp.load_state_dict(torch.load('mlp.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "\n",
    "#predict\n",
    "y_pred = mlp(x_test)\n",
    "y_pred = y_pred.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fontP = FontProperties()\n",
    "fontP.set_size('large')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('PyTorch MLP Implementation')\n",
    "# im = plt.imread(\"imshow/circumplex-model.png\",0)\n",
    "# implot = plt.imshow(im,extent=[y_train[:,0].min(),y_train[:,0].max(),y_train[:,1].min(),y_train[:,1].max()])\n",
    "\n",
    "colors = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for key, value in circumplex_model.items():\n",
    "        if tuple(y_test[i]) == value[0]:\n",
    "            colors.append(value[1])\n",
    "\n",
    "plt.scatter(y_test[:,0], y_test[:,1], color=colors, s=250, marker='x')\n",
    "plt.scatter(y_pred[:,0], y_pred[:,1], c=colors, s=15, marker='o')\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.xlabel(\"Valence\")\n",
    "plt.ylabel(\"Arousal\")\n",
    "\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "\n",
    "for key, value in circumplex_model.items():\n",
    "    plt.annotate(key, value[0])\n",
    "\n",
    "# legend\n",
    "grey_patch = mpatches.Patch(color='grey', label='neutral')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='calm')\n",
    "purple_patch = mpatches.Patch(color='purple', label='happy')\n",
    "blue_patch = mpatches.Patch(color='blue', label='sad')\n",
    "red_patch = mpatches.Patch(color='red', label='angry')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='fearful')\n",
    "green_patch = mpatches.Patch(color='green', label='disgust')\n",
    "orange_patch = mpatches.Patch(color='orange', label='surprised')\n",
    "\n",
    "plt.legend(title=\"Class Labels\",handles=[\n",
    "                    grey_patch,\n",
    "                    cyan_patch,\n",
    "                    purple_patch,\n",
    "                    blue_patch,\n",
    "                    red_patch,\n",
    "                    magenta_patch,\n",
    "                    green_patch,\n",
    "                    orange_patch],\n",
    "#                     bbox_to_anchor=(1, 1),\n",
    "                    prop=fontP)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_rfr = pd.DataFrame({'True x':y_test[:,0], \n",
    "                       'True y':y_test[:,1],\n",
    "                       'Pred x':y_pred[:,0], \n",
    "                       'Pred y':y_pred[:,1]})\n",
    "print(df_rfr)\n",
    "print()\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn MLPRegressor and RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params = {\n",
    "    'batch_size': 'auto',\n",
    "    'epsilon': 1e-08, \n",
    "    'hidden_layer_sizes': (300,), \n",
    "    'learning_rate': 'adaptive', \n",
    "    'max_iter': 100, \n",
    "    'verbose':1,\n",
    "    'n_iter_no_change':10,\n",
    "    'early_stopping':True,\n",
    "    'random_state':420,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor(**mlp_params)\n",
    "mlp.out_activation_ = 'logistic'\n",
    "mlp.fit(X_train_a, y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_mlp = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_params = {\n",
    "    'verbose': 10,\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': None,\n",
    "    'random_state': 420,\n",
    "    'min_samples_split': 4,\n",
    "    'min_samples_leaf': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init regr\n",
    "rfr = RandomForestRegressor(**rfr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "print(\"[*] Training the model...\")\n",
    "rfr.fit(X_train_a, y_train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_rfr = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('RandomForestRegressor')\n",
    "# im = plt.imread(\"imshow/circumplex-model.png\",0)\n",
    "# implot = plt.imshow(im,extent=[y_train[:,0].min(),y_train[:,0].max(),y_train[:,1].min(),y_train[:,1].max()])\n",
    "\n",
    "colors = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for key, value in circumplex_model.items():\n",
    "        if tuple(y_test[i]) == value[0]:\n",
    "            colors.append(value[1])\n",
    "\n",
    "plt.scatter(y_test[:,0], y_test[:,1], color='black', s=15, marker='x')\n",
    "plt.scatter(y_predict_rfr[:,0],y_predict_rfr[:,1], c=colors, s=15, marker='o')\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "for key, value in circumplex_model.items():\n",
    "    plt.annotate(key, value[0])\n",
    "\n",
    "# legend\n",
    "black_patch = mpatches.Patch(color='black', label='ground truth')\n",
    "grey_patch = mpatches.Patch(color='grey', label='neutral')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='calm')\n",
    "purple_patch = mpatches.Patch(color='purple', label='happy')\n",
    "blue_patch = mpatches.Patch(color='blue', label='sad')\n",
    "red_patch = mpatches.Patch(color='red', label='angry')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='fearful')\n",
    "green_patch = mpatches.Patch(color='green', label='disgust')\n",
    "orange_patch = mpatches.Patch(color='orange', label='surprised')\n",
    "\n",
    "plt.legend(handles=[black_patch,\n",
    "                    grey_patch,\n",
    "                    cyan_patch,\n",
    "                    purple_patch,\n",
    "                    blue_patch,\n",
    "                    red_patch,\n",
    "                    magenta_patch,\n",
    "                    green_patch,\n",
    "                    orange_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_rfr = pd.DataFrame({'True x':y_test[:,0], \n",
    "                        'True y':y_test[:,1],\n",
    "                        'Pred x':y_predict_rfr[:,0], \n",
    "                        'Pred y':y_predict_rfr[:,1]})\n",
    "print(df_rfr)\n",
    "print()\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_predict_rfr))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_predict_rfr))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_predict_rfr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('MLPRegressor')\n",
    "# im = plt.imread(\"imshow/circumplex-model.png\",0)\n",
    "# implot = plt.imshow(im,extent=[y_train[:,0].min(),y_train[:,0].max(),y_train[:,1].min(),y_train[:,1].max()])\n",
    "\n",
    "colors = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    for key, value in circumplex_model.items():\n",
    "        if tuple(y_test[i]) == value[0]:\n",
    "            colors.append(value[1])\n",
    "\n",
    "plt.scatter(y_test[:,0], y_test[:,1], color='black', s=15, marker='x')\n",
    "plt.scatter(y_predict_mlp[:,0],y_predict_mlp[:,1], c=colors, s=15, marker='o')\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "for key, value in circumplex_model.items():\n",
    "    plt.annotate(key, value[0])\n",
    "\n",
    "# legend\n",
    "black_patch = mpatches.Patch(color='black', label='ground truth')\n",
    "grey_patch = mpatches.Patch(color='grey', label='neutral')\n",
    "cyan_patch = mpatches.Patch(color='cyan', label='calm')\n",
    "purple_patch = mpatches.Patch(color='purple', label='happy')\n",
    "blue_patch = mpatches.Patch(color='blue', label='sad')\n",
    "red_patch = mpatches.Patch(color='red', label='angry')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='fearful')\n",
    "green_patch = mpatches.Patch(color='green', label='disgust')\n",
    "orange_patch = mpatches.Patch(color='orange', label='surprised')\n",
    "\n",
    "plt.legend(handles=[black_patch,\n",
    "                    grey_patch,\n",
    "                    cyan_patch,\n",
    "                    purple_patch,\n",
    "                    blue_patch,\n",
    "                    red_patch,\n",
    "                    magenta_patch,\n",
    "                    green_patch,\n",
    "                    orange_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df_rfr = pd.DataFrame({'True x':y_test[:,0], \n",
    "                        'True y':y_test[:,1],\n",
    "                        'Pred x':y_predict_mlp[:,0], \n",
    "                        'Pred y':y_predict_mlp[:,1]})\n",
    "print(df_rfr)\n",
    "print()\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_predict_mlp))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_predict_mlp))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_predict_mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w5b0L3dlQS1B"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "# make result directory if doesn't exist yet\n",
    "if not os.path.isdir(\"result\"):\n",
    "    os.mkdir(\"result\")\n",
    "\n",
    "pickle.dump(rfr, open(\"result/rfr.model\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ser.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
